{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "from math import sqrt\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "from wasabi import msg\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Union, List, Dict, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "ns = [sqrt(i**2) for i in range(COUNT)]\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ns), ns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend(backend='multiprocessing', n_jobs=3):\n",
    "    s = time.time()\n",
    "    ns = Parallel(n_jobs=3)(delayed(sqrt)(i**2) for i in range(COUNT))\n",
    "    print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ns), ns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from wasabi import msg\n",
    "\n",
    "import fgscraper.common.utils as spyder_utils\n",
    "from fgscraper.common.data_ingest_manager import DataIngestManager\n",
    "\n",
    "ROOT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../')\n",
    "DATA_PATH = Path(ROOT_PATH)/'data'\n",
    "data_manager = DataIngestManager(\n",
    "    raw_enterprise_path=DATA_PATH/'raw_enterprise')\n",
    "file_paths = data_manager.get_file_paths(DATA_PATH/'enterprise_ids', 'txt')\n",
    "dataset = spyder_utils.gen_dataset(file_paths)\n",
    "\n",
    "async def run(playwright):\n",
    "    if not len(file_paths):\n",
    "        msg.fail('No enterprise ids provided. Be sure to have run fg_post_spider.py before.')\n",
    "        \n",
    "    for data in dataset:\n",
    "        chromium = playwright.chromium\n",
    "        browser = await chromium.launch()\n",
    "        fpath = Path(data[0])\n",
    "        regione, provincia = fpath.stem.split('_')[1:]\n",
    "        id_lines = data[1].splitlines()\n",
    "        start_n_lines = len(id_lines)\n",
    "        msg.info(f'===== Processing files with {start_n_lines} Ids')\n",
    "        msg.info(f'regione: {regione}, provincia: {provincia}')\n",
    "        # check if raw enterprise json already exist\n",
    "        already_ids = data_manager.get_raw_enterprise_paths()\n",
    "        msg.good(f'Checking if current TXT contains already processed enterprise ...')\n",
    "        id_lines = list(set(id_lines).difference(already_ids))  # ids are unique, so sets could be used. \n",
    "        msg.good(f'id_lines reduced of {start_n_lines - len(id_lines)}')\n",
    "\n",
    "        for id_line in id_lines:\n",
    "            _url = spyder_utils.get_full_url(id_line)\n",
    "            msg.info(f'requesting page at {_url}')\n",
    "            try:\n",
    "                context = await browser.new_context()  # create a new incognite mode context\n",
    "                page = await context.new_page()\n",
    "                await page.goto(_url)\n",
    "                html = await page.inner_html('#main_content')\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "            except PlaywrightTimeoutError as e:\n",
    "                msg.fail(e)\n",
    "                continue\n",
    "\n",
    "            # get cols and values\n",
    "            data1 = spyder_utils.extract_table1_values(_soup=soup)\n",
    "            data2 = spyder_utils.extract_table2_values(_soup=soup)\n",
    "            if not data1 and not data2:\n",
    "                msg.warn(\n",
    "                    f'Problems with {_url}: neither table1 nor table2 founded.')\n",
    "                continue\n",
    "\n",
    "            full_dict = spyder_utils.get_full_dict(data1, data2)\n",
    "            # Make two dictionaries and join them\n",
    "            full_dict.update({'Regione': regione.upper(),\n",
    "                             'Provincia': provincia.upper(),\n",
    "                              'source_file': fpath.name,\n",
    "                              'EnterpriseID': id_line})\n",
    "            msg.info(f'Retrived full dict: {full_dict}')\n",
    "            # write the raw enterprise info to the disk\n",
    "            now = datetime.strftime(datetime.now(), '%Y%m%d%H%M')\n",
    "            fname_raw = f'{now}_{regione}_{provincia}_{id_line}'\n",
    "            try:\n",
    "                await data_manager.write_json(\n",
    "                    full_dict=full_dict, dir_path=DATA_PATH/'raw_enterprise', fname=fname_raw)\n",
    "            except PermissionError as e:\n",
    "                msg.fail(e)\n",
    "            await context.close()\n",
    "            \n",
    "        await browser.close()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with async_playwright() as playwright:\n",
    "        await run(playwright)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(data_path: Union[str, pathlib.Path], file_prefix: str, sort: Optional[bool] = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Util for getting a sorted/unsorted list of file paths from disk.\n",
    "\n",
    "        --Parameters\n",
    "     - data_path: (str, or Path), the directory path\n",
    "     - file_prefix: (str), the file prefixs which should be searched (txt, json, csv, etc ..)\n",
    "     - sort: (bool), if or not returning a sorted lis\n",
    "\n",
    "        --Return\n",
    "     - A list containing the founded file with prefix file_prefix\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    if isinstance(data_path, str):\n",
    "        data_path = pathlib.Path(data_path)\n",
    "\n",
    "    if not data_path.is_dir():\n",
    "        raise ValueError(f'data path {data_path} is not a regular directory.')\n",
    "    \n",
    "    file_prefix = file_prefix if not file_prefix[0] == '.' else file_prefix[1:]\n",
    "    ls = list(map(lambda x: x, data_path.glob(f'*.{file_prefix}')))\n",
    "    msg.info(f'Founded {len(ls)} file with prefix {file_prefix} at {data_path}')\n",
    "    if sort:\n",
    "        return sorted(ls)\n",
    "    return ls\n",
    "\n",
    "@lru_cache\n",
    "def get_raw_enterprise_paths():\n",
    "    enterprise_ids_paths = get_file_paths(data_path='../../data/raw_enterprise', file_prefix='json')\n",
    "    ent_ids = list(map(lambda x: str(x.stem).split('_')[3], enterprise_ids_paths)) # sorted enterprise ids\n",
    "    msg.info(f'ENT ID: {ent_ids[0]}')\n",
    "    return sorted(ent_ids)\n",
    "\n",
    "def check_raw_enterprise_exist(enterprise_id: str):\n",
    "    ent_ids = get_raw_enterprise_paths()\n",
    "    msg.info(f'Looking if {enterprise_id} has been already processed')\n",
    "    return binary_search(array=ent_ids, item=enterprise_id)\n",
    "\n",
    "def binary_search(array, item):\n",
    "    left = 0\n",
    "    right = len(array) - 1\n",
    "    if item < array[left] or item > array[right]:\n",
    "        return None\n",
    "    while left <= right:\n",
    "        mid = left + (right - left) // 2\n",
    "        el = array[mid]\n",
    "        print(el)\n",
    "        print(mid)\n",
    "        print(right)\n",
    "        print(left)\n",
    "        if el == item:\n",
    "            msg.good('found')\n",
    "            return True\n",
    "        elif el > item:\n",
    "            right = mid - 1\n",
    "        else:\n",
    "            left = mid + 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/enterprise_ids/202207211828_Abruzzo_AQUILA.txt') as f:\n",
    "    txt = f.read().splitlines()\n",
    "txt[0], txt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_ids = get_raw_enterprise_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(txt).difference(ent_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_ids[8] < ent_ids[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/final_df_prova.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_PATH = '../..'\n",
    "# DATA_PATH = Path(ROOT_PATH)/'data'\n",
    "# file_paths = sorted(list(map(lambda x: str(x), Path(DATA_PATH).glob('*.txt'))))\n",
    "# logger.info(f'Retrived {len(file_paths)} txt files in {DATA_PATH}')\n",
    "\n",
    "# BASE_URL = 'https://www.fgas.it/RicercaSezC/DettaglioImpresa?id='\n",
    "\n",
    "\n",
    "# def gen_dataset(file_paths: str):\n",
    "#     for f_path in file_paths:\n",
    "#         with open(f_path) as f:\n",
    "#             logger.info(f'yielding {f_path}')\n",
    "#             yield (f_path, f.read())\n",
    "\n",
    "\n",
    "# def get_full_url(enterprise_id: str) -> str:\n",
    "#     return BASE_URL + enterprise_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = gen_dataset(file_paths)\n",
    "\n",
    "# for data in dataset:\n",
    "#     p = Path(data[0])\n",
    "#     print(p.parent)\n",
    "#     ddd = p.stem.split('_')\n",
    "#     print(ddd[0], ddd[1], ddd[2])\n",
    "#     print(p.name)\n",
    "#     lines = data[1].splitlines()\n",
    "#     print(len(lines))\n",
    "#     for line in lines:\n",
    "#         print(line)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_dict = {\n",
    "    'FormatoReport': '3',\n",
    "    'GeneraReport': 'false',\n",
    "    'displayReport': 'display%3Anone%3B',\n",
    "    'flagAreaPub': 'True',\n",
    "    'DataFromSession': 'false',\n",
    "    'DownloadToken': '',\n",
    "    'NumRecord': '160',\n",
    "    'Nazionalita': 'I',\n",
    "    'IDRegione': '19',\n",
    "    'IstatProv': '083',\n",
    "    'Identificativo': '',\n",
    "    'RadioBtnDenominazione': 'C',\n",
    "    'Denominazione': '',\n",
    "    'NumCertProv': '',\n",
    "    'TipoSoggetto': 'I',\n",
    "    'IDAttivita_I': '',\n",
    "    'IDAttivita': ''}\n",
    "\n",
    "raw_payload = \"FormatoReport=3&GeneraReport=false&displayReport=display%3Anone%3B&flagAreaPub=True&DataFromSession=false&DownloadToken=&NumRecord=160&Nazionalita=I&IDRegione=19&IstatProv=083&Identificativo=&RadioBtnDenominazione=C&Denominazione=&NumCertProv=&TipoSoggetto=I&IDAttivita_I=&IDAttivita=\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [key + '=' + value for key, value in payload_dict.items()]\n",
    "ss = '&'.join(s)\n",
    "print(ss)\n",
    "print(raw_payload)\n",
    "assert ss == raw_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../assets/regions-ids.json') as f:\n",
    "    regions = json.load(f)\n",
    "\n",
    "with open('../../assets/provinces-ids.json') as f:\n",
    "    provinces = json.load(f)\n",
    "len(regions), len(provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/202207181700_Abruzzo_AQUILA.json'\n",
    "with open(data_path) as f:\n",
    "    d = json.loads(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b93850bb842f8947f316d4abbea9a1494c38451463ad7eee7d18c008579f1a94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
